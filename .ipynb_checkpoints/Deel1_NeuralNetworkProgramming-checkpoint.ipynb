{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning met PyTorch\n",
    "## IPynb tutorial documentatie\n",
    "Main source: https://www.youtube.com/playlist?list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG\n",
    "De eerste twee videos zijn niet heel interessant en leggen vooral achtergrond info uit over PyTorch en de cursus. De derde video legt uit hoe je pytorch installeerd. Dit is wel belangrijk, maar zeker niet moeilijk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n",
      "False\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het bovenstaande geeft aan welke versies geïnstalleerd zijn en of ze bruikbaar zijn. In mijn geval heb ik torch versie 1.0.1 en geen toegang tot cuda. Dit komt omdat ik geen NVIDEA GPU heb. Heb je dit wel, dan zou hier `True` moeten staan, mits cuda is geïnstalleerd met versie 9.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De reden dat vaak wordt gewerkt met GPU's komt vanwege de grote hoeveelheid cores. Mijn computer heeft 4 cores op de CPU, maar een GPU kan er 100 hebben. Berekeningen die parallel uitgevoerd kunnen worden kunnen verdeeld worden over meerdere cores. Parallelen berekeningen zijn berekeningen die onafhankelijk van elkaar zijn. Een laag bereken van een neural network wordt ook wel *embarisangly paralel* genoemd. <br> <br>\n",
    "Wanneer een taak simpel en klein is, gaat het gebruik van een GPU niet uitmaken. Omdat alles standaard op de CPU draait, is memory sharing nodig. Hiervoor moet je data gestuurd worden naar een plek waar alle GPUs bij kunnen. Deze transfer kost veel tijd en kan er voor zorgen dat je simpele taken met GPUs langzamer zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = nn.Linear(5, 10) # input = (input, output)\n",
    "        self.output = nn.Linear(10, 3) # input = (output previous, output nodes)\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.hidden(input)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1130,  0.2135, -0.0989], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "x = torch.randn(5)\n",
    "output = model.forward(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data terminologie\n",
    "## Verduidelijking van termen die veel gebruikt worden met neural networks\n",
    "Vaak zijn er verschillen in terminologie binnen verschillende disciplines. Zo worden er binnen computer science en wiskunde verschillende termen gebruikt voor hetzelfde concept. Een voorbeeld is:\n",
    "\n",
    "|Computer Science|Wiskunde|index|\n",
    "|-------|----|--|\n",
    "|number| scalar|0|\n",
    "|array| vector|1|\n",
    "|2d-array| matrix|2|\n",
    "|nd-array| nd-tensor|n|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voortaan gaan we alleen maar gebruik maken van tensors. Aangezeien nd-tensor de overkoepelende term is, kan je een matrix ook zien als een 2d-tensor en een scalar als een 0d-tensor. Voor versimpeling geldt: het aantal indices die je nodig hebt om een element uit de tensor te halen is gelijk aan `n`. Dit heet ook wel *rank*. Zie dit als voorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n"
     ]
    }
   ],
   "source": [
    "scalar = 4 # kan niet op geïndexeerd worden: rank = 0\n",
    "\n",
    "vector = [1, 1, 1, 1, 2, 3, 1, 4, 9, 1, 8, 27]\n",
    "vector[1] # indexeer met 1 index: rank = 1\n",
    "\n",
    "matrix = [[1, 1, 1],\n",
    "          [1, 2, 3],\n",
    "          [1, 4, 9],\n",
    "          [1, 8, 27]]\n",
    "matrix[2][1] # indexeer met 2 indexen: rank = 2\n",
    "print(scalar, vector[7], matrix[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanneer rank $\\gt 1$ kan je praten over *Axes* en *Length of Axes*. Met axes praat je over een specifieke dimensie.<br>\n",
    "`matrix[0] <- axis 1`<br>\n",
    "`matrix[1] <- axis 1`<br>\n",
    "`matrix[2] <- axis 1`<br>\n",
    "`matrix[3] <- axis 1`<br>\n",
    "Length of axis 1 = 4 <br>\n",
    "<br>\n",
    "`matrix[0][0] <- axis 2`<br>\n",
    "`matrix[0][1] <- axis 2`<br>\n",
    "`matrix[0][2] <- axis 2`<br>\n",
    "<br>\n",
    "`matrix[1][0] <- axis 2`<br>\n",
    "`matrix[1][1] <- axis 2`<br>\n",
    "`matrix[1][2] <- axis 2`<br>\n",
    "<br>\n",
    "`matrix[2][0] <- axis 2`<br>\n",
    "`matrix[2][1] <- axis 2`<br>\n",
    "`matrix[2][2] <- axis 2`<br>\n",
    "<br>\n",
    "`matrix[3][0] <- axis 2`<br>\n",
    "`matrix[3][1] <- axis 2`<br>\n",
    "`matrix[3][2] <- axis 2`<br>\n",
    "Length of axis 2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De elementen van de laatste axis zijn altijd scalars (of 0d-tensors). Dit kan je generaliseren tot:<br> Axis $k$ bij een nd-tensor bestaat uit ($n-k$)d-tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Door de lengte van alle axes te lezen wordt de *shape* van de tensor achterhaald. Dit is hoe groot de tensor is. In ons geval is `matrix`:\n",
    "- een 2d-tensor\n",
    "- met rank = 2\n",
    "- axis 1 heeft een lengte van 4\n",
    "- axis 2 heeft een lengte van 3\n",
    "- de shape is (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1],\n",
      "        [ 1,  2,  3],\n",
      "        [ 1,  4,  9],\n",
      "        [ 1,  8, 27]])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([4, 3])\n",
      "rank = 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor(matrix)\n",
    "print(t)\n",
    "print(type(t))\n",
    "print(t.shape)\n",
    "print(\"rank =\", len(t.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanuit de shape van een tensor zijn alle bovenliggende termen te achterhalen. In deep learning wordt vaak gewerkt met het *reshapen* van een tensor. Niet alleen veranderd dat hoe de tensor er uit ziet, maar daarmee ook de rank en length of axis. Een belangrijke voorwaarde voor reshapen is dat het product van de shape moet hetzelfde blijven. `t` bevat $4\\times3=12$ elementen. De mogelijke opties voor reshapen zijn:\n",
    "- $1\\times12 = 12$\n",
    "- $12\\times1 = 12$\n",
    "- $6\\times2 = 12$\n",
    "- $2\\times6 = 12$ \n",
    "- $3\\times4=12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  1,  1,  1,  2,  3,  1,  4,  9,  1,  8, 27]]),\n",
       " torch.Size([1, 12]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(1, 12), t.reshape(1, 12).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
