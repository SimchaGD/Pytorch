{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 3], dtype = np.float32)\n",
    "Y = np.array([2, 4, 6], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forwardpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model forward pass\n",
    "class NN():\n",
    "    def __init__(self, X, Y, alpha = 0.01, doBias = True):\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(Y)\n",
    "        self.w = np.random.rand()\n",
    "        if doBias:\n",
    "            self.b = np.random.rand()\n",
    "        else:\n",
    "            self.b = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.w + self.b\n",
    "\n",
    "    # Loss function\n",
    "    def loss(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        return (y_pred - y)*(y_pred-y)\n",
    "\n",
    "    # Compute gradient\n",
    "    def gradient(self, x, y):\n",
    "        return 2*x*(x*self.w-y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict (before training): 4 1.25450793419779\n",
      "\tgrad: 1.0 2.0 -3.372746032901105\n",
      "\tgrad: 2.0 4.0 -13.221164448972331\n",
      "\tgrad: 3.0 6.0 -27.367810409372723\n",
      "progress: 0  w = 0.7532441924619091  loss: 13.989600392669614\n",
      "\tgrad: 1.0 2.0 -2.493511615076182\n",
      "\tgrad: 2.0 4.0 -9.774565531098633\n",
      "\tgrad: 3.0 6.0 -20.23335064937417\n",
      "progress: 1  w = 1.078258470417399  loss: 7.646467026215458\n",
      "\tgrad: 1.0 2.0 -1.843483059165202\n",
      "\tgrad: 2.0 4.0 -7.2264535919275925\n",
      "\tgrad: 3.0 6.0 -14.958758935290117\n",
      "progress: 2  w = 1.318545426281228  loss: 4.1794230243801\n",
      "\tgrad: 1.0 2.0 -1.362909147437544\n",
      "\tgrad: 2.0 4.0 -5.342603857955172\n",
      "\tgrad: 3.0 6.0 -11.059189985967205\n",
      "progress: 3  w = 1.4961924561948272  loss: 2.2843983707550106\n",
      "\tgrad: 1.0 2.0 -1.0076150876103456\n",
      "\tgrad: 2.0 4.0 -3.9498511434325554\n",
      "\tgrad: 3.0 6.0 -8.176191866905388\n",
      "progress: 4  w = 1.62752903717431  loss: 1.248611563334668\n",
      "\tgrad: 1.0 2.0 -0.7449419256513798\n",
      "\tgrad: 2.0 4.0 -2.9201723485534092\n",
      "\tgrad: 3.0 6.0 -6.044756761505557\n",
      "progress: 5  w = 1.7246277475314136  loss: 0.6824688968666062\n",
      "\tgrad: 1.0 2.0 -0.5507445049371729\n",
      "\tgrad: 2.0 4.0 -2.1589184593537176\n",
      "\tgrad: 3.0 6.0 -4.468961210862194\n",
      "progress: 6  w = 1.7964139892829445  loss: 0.37302537383716566\n",
      "\tgrad: 1.0 2.0 -0.407172021434111\n",
      "\tgrad: 2.0 4.0 -1.5961143240217144\n",
      "\tgrad: 3.0 6.0 -3.303956650724949\n",
      "progress: 7  w = 1.8494864192447524  loss: 0.20388904192589788\n",
      "\tgrad: 1.0 2.0 -0.30102716151049513\n",
      "\tgrad: 2.0 4.0 -1.1800264731211403\n",
      "\tgrad: 3.0 6.0 -2.44265479936076\n",
      "progress: 8  w = 1.8887235035846766  loss: 0.11144212789022558\n",
      "\tgrad: 1.0 2.0 -0.2225529928306469\n",
      "\tgrad: 2.0 4.0 -0.8724077318961356\n",
      "\tgrad: 3.0 6.0 -1.8058840050249998\n",
      "progress: 9  w = 1.9177319508821944  loss: 0.060912287150846976\n",
      "\tgrad: 1.0 2.0 -0.16453609823561122\n",
      "\tgrad: 2.0 4.0 -0.6449815050835959\n",
      "\tgrad: 3.0 6.0 -1.3351117155230465\n",
      "progress: 10  w = 1.939178244070617  loss: 0.03329357394900112\n",
      "\tgrad: 1.0 2.0 -0.1216435118587662\n",
      "\tgrad: 2.0 4.0 -0.47684256648636314\n",
      "\tgrad: 3.0 6.0 -0.9870641126267721\n",
      "progress: 11  w = 1.955033745980336  loss: 0.018197676005048444\n",
      "\tgrad: 1.0 2.0 -0.08993250803932806\n",
      "\tgrad: 2.0 4.0 -0.3525354315141662\n",
      "\tgrad: 3.0 6.0 -0.7297483432343252\n",
      "progress: 12  w = 1.966755908808214  loss: 0.009946526392509933\n",
      "\tgrad: 1.0 2.0 -0.0664881823835719\n",
      "\tgrad: 2.0 4.0 -0.2606336749436018\n",
      "\tgrad: 3.0 6.0 -0.539511707133256\n",
      "progress: 13  w = 1.9754222444528184  loss: 0.00543659460963319\n",
      "\tgrad: 1.0 2.0 -0.049155511094363114\n",
      "\tgrad: 2.0 4.0 -0.19268960348990305\n",
      "\tgrad: 3.0 6.0 -0.3988674792240978\n",
      "progress: 14  w = 1.9818293703909022  loss: 0.002971546023519184\n",
      "\tgrad: 1.0 2.0 -0.03634125921819553\n",
      "\tgrad: 2.0 4.0 -0.1424577361353272\n",
      "\tgrad: 3.0 6.0 -0.2948875138001288\n",
      "progress: 15  w = 1.9865662354824387  loss: 0.0016241942620195977\n",
      "\tgrad: 1.0 2.0 -0.026867529035122573\n",
      "\tgrad: 2.0 4.0 -0.10532071381768127\n",
      "\tgrad: 3.0 6.0 -0.21801387760259772\n",
      "progress: 16  w = 1.9900682566869927  loss: 0.00088775572711917\n",
      "\tgrad: 1.0 2.0 -0.01986348662601456\n",
      "\tgrad: 2.0 4.0 -0.07786486757397704\n",
      "\tgrad: 3.0 6.0 -0.16118027587813444\n",
      "progress: 17  w = 1.9926573429877739  loss: 0.00048523150799273347\n",
      "\tgrad: 1.0 2.0 -0.014685314024452278\n",
      "\tgrad: 2.0 4.0 -0.05756643097585368\n",
      "\tgrad: 3.0 6.0 -0.1191625121200186\n",
      "progress: 18  w = 1.994571485558977  loss: 0.00026521892132756004\n",
      "\tgrad: 1.0 2.0 -0.010857028882046205\n",
      "\tgrad: 2.0 4.0 -0.042559553217621726\n",
      "\tgrad: 3.0 6.0 -0.08809827516047974\n",
      "progress: 19  w = 1.9959866341315782  loss: 0.00014496395034430877\n",
      "\tgrad: 1.0 2.0 -0.00802673173684365\n",
      "\tgrad: 2.0 4.0 -0.03146478840842626\n",
      "\tgrad: 3.0 6.0 -0.06513211200544156\n",
      "progress: 20  w = 1.9970328704530855  loss: 7.923471973355587e-05\n",
      "\tgrad: 1.0 2.0 -0.005934259093828942\n",
      "\tgrad: 2.0 4.0 -0.02326229564780924\n",
      "\tgrad: 3.0 6.0 -0.0481529519909607\n",
      "progress: 21  w = 1.9978063655204117  loss: 4.3308290070351134e-05\n",
      "\tgrad: 1.0 2.0 -0.004387268959176627\n",
      "\tgrad: 2.0 4.0 -0.017198094319972768\n",
      "\tgrad: 3.0 6.0 -0.035600055242342776\n",
      "progress: 22  w = 1.9983782197056266  loss: 2.367154190896393e-05\n",
      "\tgrad: 1.0 2.0 -0.0032435605887468633\n",
      "\tgrad: 2.0 4.0 -0.012714757507888308\n",
      "\tgrad: 3.0 6.0 -0.026319548041328744\n",
      "progress: 23  w = 1.998800998367006  loss: 1.2938444243298617e-05\n",
      "\tgrad: 1.0 2.0 -0.0023980032659878248\n",
      "\tgrad: 2.0 4.0 -0.009400172802672913\n",
      "\tgrad: 3.0 6.0 -0.019458357701534368\n",
      "progress: 24  w = 1.999113563704708  loss: 7.071923750501175e-06\n",
      "\tgrad: 1.0 2.0 -0.001772872590584118\n",
      "\tgrad: 2.0 4.0 -0.006949660555090276\n",
      "\tgrad: 3.0 6.0 -0.01438579734903378\n",
      "progress: 25  w = 1.999344647009655  loss: 3.865387877589257e-06\n",
      "\tgrad: 1.0 2.0 -0.0013107059806900523\n",
      "\tgrad: 2.0 4.0 -0.005137967444305502\n",
      "\tgrad: 3.0 6.0 -0.01063559260971303\n",
      "progress: 26  w = 1.999515489670002  loss: 2.1127523388726494e-06\n",
      "\tgrad: 1.0 2.0 -0.0009690206599959339\n",
      "\tgrad: 2.0 4.0 -0.0037985609871835635\n",
      "\tgrad: 3.0 6.0 -0.007863021243469603\n",
      "progress: 27  w = 1.9996417956989085  loss: 1.1547928918828257e-06\n",
      "\tgrad: 1.0 2.0 -0.0007164086021829164\n",
      "\tgrad: 2.0 4.0 -0.0028083217205576716\n",
      "\tgrad: 3.0 6.0 -0.005813225961553314\n",
      "progress: 28  w = 1.9997351752617514  loss: 6.311892778957254e-07\n",
      "\tgrad: 1.0 2.0 -0.0005296494764972515\n",
      "\tgrad: 2.0 4.0 -0.002076225947869048\n",
      "\tgrad: 3.0 6.0 -0.004297787712088663\n",
      "progress: 29  w = 1.999804211893116  loss: 3.4499684517418425e-07\n",
      "\tgrad: 1.0 2.0 -0.00039157621376784135\n",
      "\tgrad: 2.0 4.0 -0.0015349787579701513\n",
      "\tgrad: 3.0 6.0 -0.003177406028996188\n",
      "progress: 30  w = 1.9998552515031234  loss: 1.8856914613237637e-07\n",
      "\tgrad: 1.0 2.0 -0.00028949699375324656\n",
      "\tgrad: 2.0 4.0 -0.0011348282155125844\n",
      "\tgrad: 3.0 6.0 -0.002349094406110197\n",
      "progress: 31  w = 1.999892985699277  loss: 1.0306854503277317e-07\n",
      "\tgrad: 1.0 2.0 -0.00021402860144581481\n",
      "\tgrad: 2.0 4.0 -0.0008389921176679138\n",
      "\tgrad: 3.0 6.0 -0.001736713683573754\n",
      "progress: 32  w = 1.999920883043304  loss: 5.633543553149062e-08\n",
      "\tgrad: 1.0 2.0 -0.0001582339133920918\n",
      "\tgrad: 2.0 4.0 -0.0006202769404968222\n",
      "\tgrad: 3.0 6.0 -0.0012839732668279424\n",
      "progress: 33  w = 1.999941507884511  loss: 3.079194816931956e-08\n",
      "\tgrad: 1.0 2.0 -0.00011698423097783461\n",
      "\tgrad: 2.0 4.0 -0.00045857818543382223\n",
      "\tgrad: 3.0 6.0 -0.0009492568438496107\n",
      "progress: 34  w = 1.9999567560771134  loss: 1.6830331799462133e-08\n",
      "\tgrad: 1.0 2.0 -8.648784577314217e-05\n",
      "\tgrad: 2.0 4.0 -0.00033903235543064625\n",
      "\tgrad: 3.0 6.0 -0.0007017969757434628\n",
      "progress: 35  w = 1.9999680292488828  loss: 9.199160343044102e-09\n",
      "\tgrad: 1.0 2.0 -6.394150223432149e-05\n",
      "\tgrad: 2.0 4.0 -0.00025065068875917973\n",
      "\tgrad: 3.0 6.0 -0.0005188469257326744\n",
      "progress: 36  w = 1.99997636364005  loss: 5.0280976051218834e-09\n",
      "\tgrad: 1.0 2.0 -4.727271989990456e-05\n",
      "\tgrad: 2.0 4.0 -0.00018530906200808772\n",
      "\tgrad: 3.0 6.0 -0.00038358975835706133\n",
      "progress: 37  w = 1.9999825253554526  loss: 2.7482688185146295e-09\n",
      "\tgrad: 1.0 2.0 -3.4949289094754477e-05\n",
      "\tgrad: 2.0 4.0 -0.00013700121325221915\n",
      "\tgrad: 3.0 6.0 -0.00028359251143150743\n",
      "progress: 38  w = 1.9999870807855904  loss: 1.502154908668463e-09\n",
      "\tgrad: 1.0 2.0 -2.583842881920262e-05\n",
      "\tgrad: 2.0 4.0 -0.0001012866409713098\n",
      "\tgrad: 3.0 6.0 -0.00020966334681205012\n",
      "progress: 39  w = 1.9999904486697564  loss: 8.210511847764865e-10\n",
      "\tgrad: 1.0 2.0 -1.910266048721354e-05\n",
      "\tgrad: 2.0 4.0 -7.488242910902443e-05\n",
      "\tgrad: 3.0 6.0 -0.0001550066282582918\n",
      "progress: 40  w = 1.999992938586935  loss: 4.4877199026411175e-10\n",
      "\tgrad: 1.0 2.0 -1.4122826129892019e-05\n",
      "\tgrad: 2.0 4.0 -5.536147842910566e-05\n",
      "\tgrad: 3.0 6.0 -0.00011459826034787568\n",
      "progress: 41  w = 1.999994779412584  loss: 2.452907967124739e-10\n",
      "\tgrad: 1.0 2.0 -1.044117483184337e-05\n",
      "\tgrad: 2.0 4.0 -4.0929405340506264e-05\n",
      "\tgrad: 3.0 6.0 -8.472386905644669e-05\n",
      "progress: 42  w = 1.9999961403570763  loss: 1.3407159149198974e-10\n",
      "\tgrad: 1.0 2.0 -7.719285847418433e-06\n",
      "\tgrad: 2.0 4.0 -3.0259600521276298e-05\n",
      "\tgrad: 3.0 6.0 -6.263737308032091e-05\n",
      "progress: 43  w = 1.9999971465196709  loss: 7.328114989502562e-11\n",
      "\tgrad: 1.0 2.0 -5.706960658269367e-06\n",
      "\tgrad: 2.0 4.0 -2.2371285780309336e-05\n",
      "\tgrad: 3.0 6.0 -4.6308561566732465e-05\n",
      "progress: 44  w = 1.9999978903877509  loss: 4.005417457164493e-11\n",
      "\tgrad: 1.0 2.0 -4.219224498225316e-06\n",
      "\tgrad: 2.0 4.0 -1.653936003265244e-05\n",
      "\tgrad: 3.0 6.0 -3.423647526545892e-05\n",
      "progress: 45  w = 1.9999984403383488  loss: 2.1892900192887732e-11\n",
      "\tgrad: 1.0 2.0 -3.1193233023252276e-06\n",
      "\tgrad: 2.0 4.0 -1.2227747344439877e-05\n",
      "\tgrad: 3.0 6.0 -2.531143699968652e-05\n",
      "progress: 46  w = 1.9999988469234253  loss: 1.196627028341227e-11\n",
      "\tgrad: 1.0 2.0 -2.3061531493340226e-06\n",
      "\tgrad: 2.0 4.0 -9.04012034474988e-06\n",
      "\tgrad: 3.0 6.0 -1.8713049115604008e-05\n",
      "progress: 47  w = 1.9999991475166514  loss: 6.540550735304662e-12\n",
      "\tgrad: 1.0 2.0 -1.704966697158028e-06\n",
      "\tgrad: 2.0 4.0 -6.683469452539725e-06\n",
      "\tgrad: 3.0 6.0 -1.383478176819608e-05\n",
      "progress: 48  w = 1.9999993697488305  loss: 3.574948829627922e-12\n",
      "\tgrad: 1.0 2.0 -1.2605023389511416e-06\n",
      "\tgrad: 2.0 4.0 -4.941169168048987e-06\n",
      "\tgrad: 3.0 6.0 -1.0228220180152903e-05\n",
      "progress: 49  w = 1.9999995340477474  loss: 1.954003514367779e-12\n",
      "\tgrad: 1.0 2.0 -9.319045051192631e-07\n",
      "\tgrad: 2.0 4.0 -3.6530656597477673e-06\n",
      "\tgrad: 3.0 6.0 -7.561845917436472e-06\n",
      "progress: 50  w = 1.9999996555159083  loss: 1.0680236044295156e-12\n",
      "\tgrad: 1.0 2.0 -6.88968183393257e-07\n",
      "\tgrad: 2.0 4.0 -2.7007552780844435e-06\n",
      "\tgrad: 3.0 6.0 -5.590563425528217e-06\n",
      "progress: 51  w = 1.9999997453187774  loss: 5.837627257627755e-13\n",
      "\tgrad: 1.0 2.0 -5.093624451113499e-07\n",
      "\tgrad: 2.0 4.0 -1.9967007851562357e-06\n",
      "\tgrad: 3.0 6.0 -4.133170627085292e-06\n",
      "progress: 52  w = 1.999999811711116  loss: 3.190743346768428e-13\n",
      "\tgrad: 1.0 2.0 -3.765777680797555e-07\n",
      "\tgrad: 2.0 4.0 -1.4761848508015873e-06\n",
      "\tgrad: 3.0 6.0 -3.0557026455824143e-06\n",
      "progress: 53  w = 1.9999998607957685  loss: 1.7440036273080732e-13\n",
      "\tgrad: 1.0 2.0 -2.784084629503525e-07\n",
      "\tgrad: 2.0 4.0 -1.0913611738772033e-06\n",
      "\tgrad: 3.0 6.0 -2.259117628966578e-06\n",
      "progress: 54  w = 1.9999998970846413  loss: 9.5324139332185e-14\n",
      "\tgrad: 1.0 2.0 -2.058307173591345e-07\n",
      "\tgrad: 2.0 4.0 -8.068564127938771e-07\n",
      "\tgrad: 3.0 6.0 -1.6701927734175115e-06\n",
      "progress: 55  w = 1.9999999239134403  loss: 5.2102481332081084e-14\n",
      "\tgrad: 1.0 2.0 -1.5217311943871437e-07\n",
      "\tgrad: 2.0 4.0 -5.965186282708146e-07\n",
      "\tgrad: 3.0 6.0 -1.2347935580692138e-06\n",
      "progress: 56  w = 1.9999999437482934  loss: 2.847829056002363e-14\n",
      "\tgrad: 1.0 2.0 -1.1250341325208524e-07\n",
      "\tgrad: 2.0 4.0 -4.410133804810812e-07\n",
      "\tgrad: 3.0 6.0 -9.128976987682336e-07\n",
      "progress: 57  w = 1.9999999584124382  loss: 1.556572769718648e-14\n",
      "\tgrad: 1.0 2.0 -8.317512367739255e-08\n",
      "\tgrad: 2.0 4.0 -3.260464840337818e-07\n",
      "\tgrad: 3.0 6.0 -6.749162242414286e-07\n",
      "progress: 58  w = 1.9999999692538166  loss: 8.507950130303145e-15\n",
      "\tgrad: 1.0 2.0 -6.14923667541234e-08\n",
      "\tgrad: 2.0 4.0 -2.410500776761637e-07\n",
      "\tgrad: 3.0 6.0 -4.9897366238838e-07\n",
      "progress: 59  w = 1.9999999772689776  loss: 4.6502944131997335e-15\n",
      "\tgrad: 1.0 2.0 -4.5462044795385737e-08\n",
      "\tgrad: 2.0 4.0 -1.7821121645056337e-07\n",
      "\tgrad: 3.0 6.0 -3.6889721677368925e-07\n",
      "progress: 60  w = 1.9999999831946824  loss: 2.5417682464992718e-15\n",
      "\tgrad: 1.0 2.0 -3.3610635163938696e-08\n",
      "\tgrad: 2.0 4.0 -1.317536906242367e-07\n",
      "\tgrad: 3.0 6.0 -2.727301371407975e-07\n",
      "progress: 61  w = 1.999999987575627  loss: 1.3892854523053341e-15\n",
      "\tgrad: 1.0 2.0 -2.4848746171812763e-08\n",
      "\tgrad: 2.0 4.0 -9.74070850645603e-08\n",
      "\tgrad: 3.0 6.0 -2.016326678955238e-07\n",
      "progress: 62  w = 1.9999999908145119  loss: 7.59358744870813e-16\n",
      "\tgrad: 1.0 2.0 -1.8370976295756236e-08\n",
      "\tgrad: 2.0 4.0 -7.201422747016295e-08\n",
      "\tgrad: 3.0 6.0 -1.4906945011716743e-07\n",
      "progress: 63  w = 1.9999999932090582  loss: 4.150520068179015e-16\n",
      "\tgrad: 1.0 2.0 -1.3581883617774793e-08\n",
      "\tgrad: 2.0 4.0 -5.324098317771586e-08\n",
      "\tgrad: 3.0 6.0 -1.1020883405876702e-07\n",
      "progress: 64  w = 1.9999999949793754  loss: 2.2686003411860324e-16\n",
      "\tgrad: 1.0 2.0 -1.0041249165482213e-08\n",
      "\tgrad: 2.0 4.0 -3.936169612472895e-08\n",
      "\tgrad: 3.0 6.0 -8.14787117775495e-08\n",
      "progress: 65  w = 1.999999996288192  loss: 1.239976707405251e-16\n",
      "\tgrad: 1.0 2.0 -7.423615944190942e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad: 2.0 4.0 -2.9100574394647083e-08\n",
      "\tgrad: 3.0 6.0 -6.023819310030376e-08\n",
      "progress: 66  w = 1.9999999972558158  loss: 6.77749193077586e-17\n",
      "\tgrad: 1.0 2.0 -5.488368426398438e-09\n",
      "\tgrad: 2.0 4.0 -2.1514404480171834e-08\n",
      "\tgrad: 3.0 6.0 -4.4534818499641915e-08\n",
      "progress: 67  w = 1.9999999979711918  loss: 3.7044565599204024e-17\n",
      "\tgrad: 1.0 2.0 -4.057616465047431e-09\n",
      "\tgrad: 2.0 4.0 -1.5905856187714562e-08\n",
      "\tgrad: 3.0 6.0 -3.2925123960581004e-08\n",
      "progress: 68  w = 1.9999999985000778  loss: 2.0247901087056507e-17\n",
      "\tgrad: 1.0 2.0 -2.9998443729084556e-09\n",
      "\tgrad: 2.0 4.0 -1.175938990627401e-08\n",
      "\tgrad: 3.0 6.0 -2.4341936466498737e-08\n",
      "progress: 69  w = 1.9999999988910895  loss: 1.1067144329015544e-17\n",
      "\tgrad: 1.0 2.0 -2.2178210379308894e-09\n",
      "\tgrad: 2.0 4.0 -8.693858077890582e-09\n",
      "\tgrad: 3.0 6.0 -1.7996281798104974e-08\n",
      "progress: 70  w = 1.9999999991801691  loss: 6.049101712083296e-18\n",
      "\tgrad: 1.0 2.0 -1.6396617397163027e-09\n",
      "\tgrad: 2.0 4.0 -6.427473664416539e-09\n",
      "\tgrad: 3.0 6.0 -1.3304866541830052e-08\n",
      "progress: 71  w = 1.9999999993938893  loss: 3.3063318132341813e-18\n",
      "\tgrad: 1.0 2.0 -1.212221434343519e-09\n",
      "\tgrad: 2.0 4.0 -4.7519073120838584e-09\n",
      "\tgrad: 3.0 6.0 -9.83644632412961e-09\n",
      "progress: 72  w = 1.9999999995518951  loss: 1.807182500985325e-18\n",
      "\tgrad: 1.0 2.0 -8.962097730602636e-10\n",
      "\tgrad: 2.0 4.0 -3.5131417774891815e-09\n",
      "\tgrad: 3.0 6.0 -7.272204172181773e-09\n",
      "progress: 73  w = 1.9999999996687108  loss: 9.877737980722456e-19\n",
      "\tgrad: 1.0 2.0 -6.625784365610343e-10\n",
      "\tgrad: 2.0 4.0 -2.597307258156434e-09\n",
      "\tgrad: 3.0 6.0 -5.376424638825483e-09\n",
      "progress: 74  w = 1.999999999755074  loss: 5.398993857185719e-19\n",
      "\tgrad: 1.0 2.0 -4.898521588359017e-10\n",
      "\tgrad: 2.0 4.0 -1.920220427109598e-09\n",
      "\tgrad: 3.0 6.0 -3.974857776256613e-09\n",
      "progress: 75  w = 1.9999999998189233  loss: 2.950992127181178e-19\n",
      "\tgrad: 1.0 2.0 -3.621534183650965e-10\n",
      "\tgrad: 2.0 4.0 -1.419641293409768e-09\n",
      "\tgrad: 3.0 6.0 -2.938657317486104e-09\n",
      "progress: 76  w = 1.9999999998661278  loss: 1.612958273858645e-19\n",
      "\tgrad: 1.0 2.0 -2.677444932430717e-10\n",
      "\tgrad: 2.0 4.0 -1.0495586622027986e-09\n",
      "\tgrad: 3.0 6.0 -2.1725874432831915e-09\n",
      "progress: 77  w = 1.9999999999010267  loss: 8.816138073368086e-20\n",
      "\tgrad: 1.0 2.0 -1.979465480417275e-10\n",
      "\tgrad: 2.0 4.0 -7.759499709436568e-10\n",
      "\tgrad: 3.0 6.0 -1.6062138286088157e-09\n",
      "progress: 78  w = 1.9999999999268279  loss: 4.818764705747953e-20\n",
      "\tgrad: 1.0 2.0 -1.4634427003556993e-10\n",
      "\tgrad: 2.0 4.0 -5.736691122137927e-10\n",
      "\tgrad: 3.0 6.0 -1.187492770782228e-09\n",
      "progress: 79  w = 1.999999999945903  loss: 2.6338247087161744e-20\n",
      "\tgrad: 1.0 2.0 -1.081938982849806e-10\n",
      "\tgrad: 2.0 4.0 -4.24119406261525e-10\n",
      "\tgrad: 3.0 6.0 -8.779270643799464e-10\n",
      "progress: 80  w = 1.9999999999600055  loss: 1.439595257377263e-20\n",
      "\tgrad: 1.0 2.0 -7.998890438898343e-11\n",
      "\tgrad: 2.0 4.0 -3.13557180220414e-10\n",
      "\tgrad: 3.0 6.0 -6.490648019052969e-10\n",
      "progress: 81  w = 1.9999999999704314  loss: 7.86874228135428e-21\n",
      "\tgrad: 1.0 2.0 -5.913713962968359e-11\n",
      "\tgrad: 2.0 4.0 -2.3181812025541149e-10\n",
      "\tgrad: 3.0 6.0 -4.798614838819049e-10\n",
      "progress: 82  w = 1.9999999999781395  loss: 4.300909290792278e-21\n",
      "\tgrad: 1.0 2.0 -4.3721026798948515e-11\n",
      "\tgrad: 2.0 4.0 -1.7138646057901497e-10\n",
      "\tgrad: 3.0 6.0 -3.547668825376604e-10\n",
      "progress: 83  w = 1.9999999999838383  loss: 2.3508592189862607e-21\n",
      "\tgrad: 1.0 2.0 -3.232347722814666e-11\n",
      "\tgrad: 2.0 4.0 -1.2670753335441987e-10\n",
      "\tgrad: 3.0 6.0 -2.622861927648046e-10\n",
      "progress: 84  w = 1.9999999999880513  loss: 1.2849352027338921e-21\n",
      "\tgrad: 1.0 2.0 -2.389732856045157e-11\n",
      "\tgrad: 2.0 4.0 -9.367795428261161e-11\n",
      "\tgrad: 3.0 6.0 -1.9391421801628894e-10\n",
      "progress: 85  w = 1.9999999999911662  loss: 7.02327789565759e-22\n",
      "\tgrad: 1.0 2.0 -1.766764512467489e-11\n",
      "\tgrad: 2.0 4.0 -6.925660045453697e-11\n",
      "\tgrad: 3.0 6.0 -1.4336265508063661e-10\n",
      "progress: 86  w = 1.999999999993469  loss: 3.83894110816012e-22\n",
      "\tgrad: 1.0 2.0 -1.3061995929319892e-11\n",
      "\tgrad: 2.0 4.0 -5.120348589571222e-11\n",
      "\tgrad: 3.0 6.0 -1.0598988353649474e-10\n",
      "progress: 87  w = 1.9999999999951714  loss: 2.098497014493632e-22\n",
      "\tgrad: 1.0 2.0 -9.657163957399462e-12\n",
      "\tgrad: 2.0 4.0 -3.785594060445874e-11\n",
      "\tgrad: 3.0 6.0 -7.835865289962385e-11\n",
      "progress: 88  w = 1.9999999999964302  loss: 1.1469671845997851e-22\n",
      "\tgrad: 1.0 2.0 -7.139622226759457e-12\n",
      "\tgrad: 2.0 4.0 -2.7986502004750946e-11\n",
      "\tgrad: 3.0 6.0 -5.793232560336037e-11\n",
      "progress: 89  w = 1.9999999999973608  loss: 6.268241105687484e-23\n",
      "\tgrad: 1.0 2.0 -5.278444348277844e-12\n",
      "\tgrad: 2.0 4.0 -2.0691004465334117e-11\n",
      "\tgrad: 3.0 6.0 -4.282973975477944e-11\n",
      "progress: 90  w = 1.999999999998049  loss: 3.425873027802862e-23\n",
      "\tgrad: 1.0 2.0 -3.9022118869525e-12\n",
      "\tgrad: 2.0 4.0 -1.5296208744075557e-11\n",
      "\tgrad: 3.0 6.0 -3.1665337019148865e-11\n",
      "progress: 91  w = 1.9999999999985576  loss: 1.872470536891354e-23\n",
      "\tgrad: 1.0 2.0 -2.8848035071860068e-12\n",
      "\tgrad: 2.0 4.0 -1.1308287639621994e-11\n",
      "\tgrad: 3.0 6.0 -2.34106067864559e-11\n",
      "progress: 92  w = 1.9999999999989335  loss: 1.0235000084143116e-23\n",
      "\tgrad: 1.0 2.0 -2.1329604749098507e-12\n",
      "\tgrad: 2.0 4.0 -8.361311643056979e-12\n",
      "\tgrad: 3.0 6.0 -1.730882104311604e-11\n",
      "progress: 93  w = 1.9999999999992117  loss: 5.5900608564569636e-24\n",
      "\tgrad: 1.0 2.0 -1.5765166949677223e-12\n",
      "\tgrad: 2.0 4.0 -6.179945444273471e-12\n",
      "\tgrad: 3.0 6.0 -1.2795098314200004e-11\n",
      "progress: 94  w = 1.9999999999994171  loss: 3.058382963966559e-24\n",
      "\tgrad: 1.0 2.0 -1.1657341758564144e-12\n",
      "\tgrad: 2.0 4.0 -4.568789790937444e-12\n",
      "\tgrad: 3.0 6.0 -9.453771099288133e-12\n",
      "progress: 95  w = 1.9999999999995692  loss: 1.6700382578755125e-24\n",
      "\tgrad: 1.0 2.0 -8.615330671091215e-13\n",
      "\tgrad: 2.0 4.0 -3.376854351699876e-12\n",
      "\tgrad: 3.0 6.0 -6.991740519879386e-12\n",
      "progress: 96  w = 1.9999999999996816  loss: 9.133242234031622e-25\n",
      "\tgrad: 1.0 2.0 -6.368239269249898e-13\n",
      "\tgrad: 2.0 4.0 -2.495781359357352e-12\n",
      "\tgrad: 3.0 6.0 -5.169198402654729e-12\n",
      "progress: 97  w = 1.9999999999997646  loss: 4.9857981362231e-25\n",
      "\tgrad: 1.0 2.0 -4.707345624410664e-13\n",
      "\tgrad: 2.0 4.0 -1.8456347561368602e-12\n",
      "\tgrad: 3.0 6.0 -3.820943561549939e-12\n",
      "progress: 98  w = 1.999999999999826  loss: 2.727439248147335e-25\n",
      "\tgrad: 1.0 2.0 -3.481659405224491e-13\n",
      "\tgrad: 2.0 4.0 -1.3642420526593924e-12\n",
      "\tgrad: 3.0 6.0 -2.8244073746463982e-12\n",
      "progress: 99  w = 1.9999999999998712  loss: 1.4927220479044596e-25\n",
      "Predict (after training): 4 7.999999999999485\n"
     ]
    }
   ],
   "source": [
    "net = NN(X, Y, doBias = False)\n",
    "print(\"Predict (before training):\", 4, net.forward(4))\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(net.X, net.Y):\n",
    "        grad = net.gradient(x_val, y_val)\n",
    "        net.w = net.w - alpha*grad\n",
    "        print(\"\\tgrad:\", x_val, y_val, grad)\n",
    "        l = net.loss(x_val, y_val)\n",
    "    print(\"progress:\", epoch, \" w =\", net.w, \" loss:\", l)\n",
    "\n",
    "print(\"Predict (after training):\", 4, net.forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model backwardpass\n",
    "class NN():\n",
    "    def __init__(self, X, Y, alpha = 0.01, doBias = True):\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(Y)\n",
    "        self.w = w = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
    "        if doBias:\n",
    "            self.b = np.random.rand()\n",
    "        else:\n",
    "            self.b = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x * self.w + self.b\n",
    "\n",
    "    # Loss function\n",
    "    def loss(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "        return (y_pred - y)*(y_pred-y)\n",
    "\n",
    "    # Compute gradient\n",
    "    def gradient(self, x, y):\n",
    "        return 2*x*(x*self.w-y)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
