{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorbeeld voor CNN's \n",
    "Een klein voorbeeldje voor de basis van CNN's. Het idee is om de terminologie uit deel 1 hier toe te passen. Laten we beginnen bij de input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De input van een CNN heeft vaak rank = 4. Dit betekent dat we te maken hebben met axis 0, axis 1, axis 2 en axis 3 (oftwel A0, A1, A2 en A3). Laten we de input van achter naar voor bekijken:\n",
    "- (A2, A3) zijn de hoogte en breedte van een afbeelding. Deze worden vaak gekozen als $24\\times24$ of $224\\times224$ pixels.\n",
    "- A1 staat voor de kleur kanalen. Een kleuren afbeelding bestaat uit 3 kanalen Roodwaardes, Groenwaardes en Blauwwaardes (oftewel RGB). Een zwartwit afbeelding bestaat uit 1 kanaal, namelijk Grijswaardes (of grayscale).<br> <br>\n",
    "Door A1, A2 en A3 te combineren kan je een individuele pixelwaarde (lees scalar) krijgen. De laatste axis(A0) staat voor de Batchgroote. Om je data te trainen gebruik je niet 1 afbeelding, maar heel veel afbeeldingen --> Batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wanneer je 1 grayscale afbeelding van 28x28 (shape: [1, 1, 28, 28]) hebt en die door een CNN layer gaat zal dit zorgen voor een verandering van pixelwaardes, maar misschien ook de lengte van A1, A2 en A3. Dit ligt aan het aantal filters dat over de afbeelding gaat. Gaan hier 3 filters overheen dan zal de output 3 kanalen hebben. De resultaten van filters over een afbeelding heten *feature maps*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout) # voor meer info-> zie wiki pagina: stride of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let op bij PyTorch tensors dat de data type hetzelfde meoten zijn. Hieronder staat een voorbeeld waarbij t1 een integerbased tensor is en t2 een floatbased tensor. Die twee variabelen mag je dus niet optellen. Hetzelfde geldt voor devices, maar ik heb geen gpu dus dat kan ik niet testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected type torch.FloatTensor but got torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-885484029915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: expected type torch.FloatTensor but got torch.LongTensor"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([1., 2., 3.])\n",
    "print(t1.dtype, t2.dtype)\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating options using data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als we data willen inladen en converteren naar een tensor, dan zijn daar meerdere mogelijkheden voor. Aanschouw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1, 2, 3])\n",
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)\n",
    "print(\"{}\\n{}\\n{}\\n{}\".format(t1, t2, t3, t4 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet dat `torch.Tensor(data)` een andere output geeft dan de andere functies. Dit is een *class constructor* terwijl de andere *factory functions* zijn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating options without data\n",
    "Genereer tensors met voorgeprogrammeerde functies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]]) \n",
      "\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]]) \n",
      "\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) \n",
      "\n",
      "tensor([[0.6412, 0.1458],\n",
      "        [0.5279, 0.9315]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.eye(2), \"\\n\")\n",
    "print(torch.zeros(2, 2), \"\\n\")\n",
    "print(torch.ones(2, 2), \"\\n\")\n",
    "print(torch.rand(2, 2), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating PyTorch tensors - Best options\n",
    "Hierboven was aangegeven welke verschillende manieren er zijn om data in te laden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een van die verschillen waren de data types. `torch.Tensor()` gebruikt de defautl datatype (`torch.float32`). Bij de factory functions wordt de datatype overgenomen tenzij anders gespecificeerd wordt (gebruik argument `dtype =`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een belangrijke eigenschap van deze functies is te zien in het volgende voorbeeld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2, 3])\n",
    "t1 = torch.Tensor(data)\n",
    "t2 = torch.tensor(data)\n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([1., 2., 3.])\n",
      "t2: tensor([1, 2, 3], dtype=torch.int32)\n",
      "t3: tensor([0, 0, 0], dtype=torch.int32)\n",
      "t4: tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "data[0] = 0\n",
    "data[1] = 0\n",
    "data[2] = 0\n",
    "print(\"t1: {}\".format(t1))\n",
    "print(\"t2: {}\".format(t2))\n",
    "print(\"t3: {}\".format(t3))\n",
    "print(\"t4: {}\".format(t4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier is te zien dat t1 en t2 de waardes hebben behouden. Terwijl t3 en t4 de veranderingen hebben overgenomen. Dit komt doordat `torch.Tensor()` en `torch.tensor()` een kopie maken van het geheugen. `torch.as_tensor()` en `torch.from_numpy()` delen het geheugen met de variabel waarvan ze een tensor hebben gemaakt. Het voordeel hiervan is de snelheid waarmee deze functies uitgevoerd kunnen worden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dus met al deze verschillende opties. Welke moeten we nu gebruiken? Ligt aan de situatie. \n",
    "- All day use: `torch.tensor()`\n",
    "- Tuning voor performence `torch.as_tensor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is goed om eerst je code te schrijven zodat die werkt met `torch.tensor()` en daarna code te optimaliseren en bottlenecks er uit halen met `torch.as_tensor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operation types\n",
    "De hoogste groepering van tensor operations vallen te groeperen in 4 soorten:\n",
    "1. Reshaping operations\n",
    "2. Element-wise operations\n",
    "3. Reduction operations\n",
    "4. Access operations\n",
    "<br>\n",
    "Reshaping operations zorgen ervoor dat we iets van houvast hebben als het gaat om de abstracte tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2],\n",
    "    [3, 3, 3 ,3]\n",
    "], dtype = torch.float32)\n",
    "t.shape, len(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(12), 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(t.shape).prod(), t.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals eerder vermeld, moet de product van de shape gelijk staan aan het aantal elementen in de matrix. Je kan zelfs buiten de rank gaan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank 2\n",
    "t1_12 = t.reshape(1, 12)\n",
    "t12_1 = t.reshape(12, 1)\n",
    "t3_4 = t.reshape(3, 4)\n",
    "t4_3 = t.reshape(4, 3)\n",
    "t2_6 = t.reshape(2, 6)\n",
    "t6_2 = t.reshape(6, 2)\n",
    "\n",
    "# Rank 3\n",
    "t2_2_3 = t.reshape(2, 2, 3)\n",
    "t3_2_2 = t.reshape(3, 2, 2)\n",
    "t2_3_2 = t.reshape(2, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squeezing verwijderd alle axis met lengte = 1. Unsqueezing doet het omgekeerde op bepaalde dimensie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(t1_12.shape)\n",
    "print(t1_12.squeeze())\n",
    "print(t1_12.squeeze().shape)\n",
    "print(t1_12.squeeze().unsqueeze(dim = 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Flatten Operation Visualized - Tensor Batch Processing for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we 3 hypothetische afbeeldingen genereren. Deze zetten we gelijk in een tensor waarbij elke afbeelding een batch element is (A0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.ones(4, 4)\n",
    "t2 = torch.ones(4, 4)*2\n",
    "t3 = torch.ones(4, 4)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((t1, t2, t3))\n",
    "t.shape #Size([batch, hight, width]) Size([A0, A2, A3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat PyTorch een kleuren kanaal vereist, moeten we onze tensor reshapen. Tussen A0 en A2 moet A1 geplaatst worden. Op het moment hebben we maar 1 kleuren kanaal (grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.],\n",
       "          [2., 2., 2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.],\n",
       "          [3., 3., 3., 3.]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.reshape(3, 1, 4, 4)\n",
    "t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Uitgeschreven dataset.\n",
    "Batch A0\n",
    "[   Kanaal A1\n",
    "    [   Hoogte A2\n",
    "        [   Breedte A3\n",
    "            [1, 1, 1, 1],\n",
    "            [1, 1, 1, 1],\n",
    "            [1, 1, 1, 1],\n",
    "            [1, 1, 1, 1]\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [2, 2, 2, 2],\n",
    "            [2, 2, 2, 2],\n",
    "            [2, 2, 2, 2],\n",
    "            [2, 2, 2, 2] \n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [3, 3, 3, 3],\n",
    "            [3, 3, 3, 3],\n",
    "            [3, 3, 3, 3],\n",
    "            [3, 3, 3, 3] \n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(t[0]) # Get first batch element\n",
    "print(t[0][0]) # Get first color channel\n",
    "print(t[0][0][0]) # Get first row\n",
    "print(t[0][0][0][0]) # Get first col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening t op verschillende manieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
      "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3.,\n",
      "        3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1, -1)[0])\n",
    "print(t.reshape(-1))\n",
    "print(t.view(t.numel()))\n",
    "print(t.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het vervelende hiervan is dat onze batches verdwenen zijn. Onze 3 afbeeldingen wordt nu gezien als een grote afbeelding. Dit maakt het onmogelijk om te trainen. Hieronder de oplossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]])\n",
      "torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "print(t.flatten(start_dim=1))\n",
    "print(t.flatten(start_dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(3, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element-/Component-/Point-wise operations\n",
    "## Element-wise operations\n",
    "Een element-wise (ook wel component- of point-wise) operatie is een operatie tussen twee tensoren waar gebruikt wordt gemaakt van elementen op dezelfde positie in die tensoren. <br>\n",
    "$\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix} \\times\n",
    "\\begin{pmatrix}\n",
    "3\\\\\n",
    "2\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix} = 4 \\rightarrow$ element wise <br>\n",
    "$\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "3\\\\\n",
    "2\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "4\\\\\n",
    "3\\\\\n",
    "3\\\\\n",
    "4\n",
    "\\end{pmatrix} \\rightarrow$ element wise <br>\n",
    "$\\begin{pmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & 1 & 0\n",
    "\\end{pmatrix} = 4 \\rightarrow$ non element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "], dtype = torch.float32)\n",
    "t2 = torch.tensor([\n",
    "    [9, 8],\n",
    "    [7, 6]\n",
    "],  dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element wise operaties zijn dus alleen mogelijk op tensors met dezelfde Shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(9.))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0][0], t2[0][0] # dezelfde positie op een andere tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Het gekke is dat de volgende operations wel werken, maar niet dezelfde Shape hebben. Dit komt omdat deze tensors met andere shapes worden gebroadcast wanneer mogelijk. Broadcasten kan ook worden gedaan door loops te schrijven, maar in numpy worden deze loops uitgevoerd in C zodat dit efficienter en dus sneller gebeurd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[3., 6.],\n",
      "        [5., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1+2)\n",
    "t3 = torch.tensor([2,4], dtype = torch.float32)\n",
    "print(t1+t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[3., 6.],\n",
      "        [5., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1 + torch.tensor(np.broadcast_to(2, t1.shape), dtype = torch.float32))\n",
    "t3 = torch.tensor(np.broadcast_to(t3.numpy(), t1.shape))\n",
    "print(t1 + t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison operations\n",
    "Een tensor met dezelfde Shape wordt gereturned met elk element een 0 of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0, 5, 7],\n",
    "    [6, 0, 7],\n",
    "    [0, 8, 0]\n",
    "], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 1]], dtype=torch.uint8)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.uint8)\n",
      "tensor([[0, 1, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 1, 0]], dtype=torch.uint8)\n",
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 1]], dtype=torch.uint8)\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(t.eq(0)) # equal\n",
    "print(t.ge(0)) # greater or equal\n",
    "print(t.gt(0)) # greater\n",
    "print(t.le(0)) # less or equal\n",
    "print(t.lt(0)) # less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 5., 7.],\n",
      "        [6., 0., 7.],\n",
      "        [0., 8., 0.]])\n",
      "tensor([[0.0000, 2.2361, 2.6458],\n",
      "        [2.4495, 0.0000, 2.6458],\n",
      "        [0.0000, 2.8284, 0.0000]])\n",
      "tensor([[-0., -5., -7.],\n",
      "        [-6., -0., -7.],\n",
      "        [-0., -8., -0.]])\n",
      "tensor([[0., 5., 7.],\n",
      "        [6., 0., 7.],\n",
      "        [0., 8., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(t.abs())\n",
    "print(t.sqrt())\n",
    "print(t.neg())\n",
    "print(t.neg().abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reductie operations zijn bijna gelijk als element-wise operaties. Het verschil is dat de functie binnen 1 tensor wordt uitgevoerd ipv met een andere tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [0, 1, 0],\n",
    "    [2, 0, 2],\n",
    "    [0, 3, 0]\n",
    "], dtype = torch.float32)\n",
    "print(t.sum())\n",
    "print(t.sum().numel() < t.numel()) # Het aantal elementen is verminderd door de sum functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.8889)\n",
      "tensor(1.1667)\n"
     ]
    }
   ],
   "source": [
    "print(t.prod())\n",
    "print(t.mean())\n",
    "print(t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n",
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 1, 1, 1],\n",
    "    [2, 2, 2, 2],\n",
    "    [3, 3, 3 ,3]\n",
    "], dtype = torch.float32)\n",
    "print(t.sum(dim = 0))\n",
    "print(t.sum(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim = 0) ==  t[0] + t[1] + t[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim = 1) ==  torch.tensor([t[0].sum(), t[1].sum(), t[2].sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De ArgMax is een reductie functie die de index van de maximale waarde ophaalt. Als voorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n",
      "tensor(5.)\n",
      "tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])\n",
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 0, 0, 2],\n",
    "    [0, 3, 3, 0],\n",
    "    [4, 0, 0, 5]\n",
    "], dtype = torch.float32)\n",
    "print(t.argmax())\n",
    "print(t.max())\n",
    "print(t.flatten())\n",
    "print(t.flatten()[t.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wil je de maximale waarde van een bepaalde as? Gebruik dan wederom de \"dim\" argument. Bij `t.max(dim = 0)` wordend de indices automatisch gegeven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2])) tensor([2, 1, 1, 2])\n",
      "(tensor([2., 3., 5.]), tensor([3, 2, 3])) tensor([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t.max(dim = 0), t.argmax(dim = 0))\n",
    "print(t.max(dim = 1), t.argmax(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de waardes uit de tensor te halen gebruik je `.item` voor scalars en `.tolist` voor vectoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "], dtype = torch.float32)\n",
    "print(t.mean())\n",
    "print(t.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 5.0, 6.0]\n",
      "[4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(t.mean(dim = 0).tolist())\n",
    "print(t.mean(dim = 0).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
